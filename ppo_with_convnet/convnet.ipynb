{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook for the neural network environment wrappers\n",
    "This notebook can be uplaoded to Google Colab, etc. to run the neural networks in a cloud compute environment. It is a direct copy of the Python files in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n",
    "%pip install gymnasium\n",
    "%pip install flappy-bird-gymnasium\n",
    "%pip install stable-baselines3\n",
    "%pip install numpy\n",
    "%pip install torch \n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "class BaseWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def pass_through_model(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        observation = self.pass_through_model()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        observation = self.pass_through_model()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "class ResNetWrapper(BaseWrapper):\n",
    "    def pass_through_model(self):\n",
    "        image = self.env.render()\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        image_tensor = normalize(transforms.ToTensor()(image))\n",
    "\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        # removing the last layer for inference\n",
    "        resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            observation = resnet50(image_tensor.unsqueeze(0)) # unsqueeze makes the single image a batch of 1\n",
    "        observation = observation.flatten()\n",
    "\n",
    "        return observation\n",
    "    \n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fully_connected = torch.nn.Linear(32 * 112 * 112, 200)\n",
    "\n",
    "        # Initialize weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, torch.nn.Conv2d):\n",
    "                # Weights are initialized to a constant value of 0.01\n",
    "                torch.nn.init.constant_(module.weight, 0.01) \n",
    "                if module.bias is not None:\n",
    "                    # Biases are initialized to a constant value of 0.01 as well\n",
    "                    torch.nn.init.constant_(module.bias, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = self.pool(torch.nn.functional.relu(self.conv(x)))\n",
    "       return x\n",
    "\n",
    "    \n",
    "class ConvNetWrapper(BaseWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(200,), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def pass_through_model(self):\n",
    "        image = self.env.render()\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        image_tensor = normalize(transforms.ToTensor()(image))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            observation = ConvNet().forward(image_tensor.unsqueeze(0))\n",
    "        observation.flatten()\n",
    "\n",
    "        return observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrapped_env():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "    wrapped_env = ConvNetWrapper(env)\n",
    "    return wrapped_env\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(make_wrapped_env, n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=100)\n",
    "model.save(\"ppo_flappybird\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
